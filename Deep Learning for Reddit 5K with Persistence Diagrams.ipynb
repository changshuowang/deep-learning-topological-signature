{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this notebook, we generate the results of the reddit-5K experiment of the article with precomputed persistence diagrams obtained from the authors code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do not forget to move the .so files to the dist-packages repo of your python version! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy              as np\n",
    "np.set_printoptions(threshold='nan')\n",
    "import math       \n",
    "import sys\n",
    "from   random             import shuffle   \n",
    "import matplotlib.pyplot  as plt\n",
    "import matplotlib.image   as mpimg\n",
    "import h5py\n",
    "\n",
    "import tensorflow as tf\n",
    "import _persistence_vector_grad\n",
    "import _persistence_vector_essential_grad\n",
    "persistence_vector_module = tf.load_op_library('persistence_vector.so')\n",
    "persistence_vector_essential_module = tf.load_op_library('persistence_vector_essential.so')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data reading\n",
    "This code assumes that precomputed persistence diagrams are available in a h5 file called reddit_5K.h5 in a repo called reddit_5K. This file can be obtained by running code provided by the authors, or by computing the diagrams manually with Gudhi using the notebook \"Persistence Diagram Computations for Graphs\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File(\"reddit_5K/reddit_5K.h5\")\n",
    "num_labels = 5\n",
    "\n",
    "D0     = []\n",
    "C0     = []\n",
    "D0i    = []\n",
    "C0i    = []\n",
    "D1i    = []\n",
    "C1i    = []\n",
    "labels = []\n",
    "\n",
    "for j in range(1,6):\n",
    "    \n",
    "    for index in f['data_views']['DegreeVertexFiltration_dim_0'][str(j)].keys():\n",
    "\n",
    "        diag0  = f['data_views']['DegreeVertexFiltration_dim_0'][str(j)][index]\n",
    "        diag0i = f['data_views']['DegreeVertexFiltration_dim_0_essential'][str(j)][index]\n",
    "        diag1i = f['data_views']['DegreeVertexFiltration_dim_1_essential'][str(j)][index]\n",
    "        num0  = diag0.shape[0]\n",
    "        num0i = diag0i.shape[0]\n",
    "        num1i = diag1i.shape[0]\n",
    "        \n",
    "        if(num0 > 0 and num0i > 0 and num1i > 0):\n",
    "            D0.append(diag0)\n",
    "            C0.append(num0)\n",
    "            D0i.append(diag0i[:,0])\n",
    "            C0i.append(num0i)\n",
    "            D1i.append(diag1i[:,0])\n",
    "            C1i.append(num1i)\n",
    "\n",
    "            vector_lab = np.zeros(num_labels)\n",
    "            vector_lab[j-1] = 1\n",
    "            labels.append(vector_lab)\n",
    "                \n",
    "num_pts = len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of the network as described in the article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network parameters as decribed in the article\n",
    "num_gaussians0  = 150\n",
    "num_gaussians0i = 50\n",
    "num_gaussians1i = 50\n",
    "keep_prob       = 0.9\n",
    "nu_tensor0      = tf.Variable([[0.01]], trainable=False)\n",
    "\n",
    "# Random initialization of Gaussians\n",
    "mu0     = tf.Variable(tf.random_uniform([num_gaussians0,2]))\n",
    "sigma0  = tf.Variable(np.float32(3*np.ones([num_gaussians0,2])))\n",
    "Gauss0  = tf.concat([mu0,sigma0],1)\n",
    "\n",
    "mu0i     = tf.Variable(tf.random_uniform([num_gaussians0i,1]))\n",
    "sigma0i  = tf.Variable(np.float32(3*np.ones([num_gaussians0i,1])))\n",
    "Gauss0i  = tf.concat([mu0i,sigma0i],1)\n",
    "\n",
    "mu1i     = tf.Variable(tf.random_uniform([num_gaussians1i,1]))\n",
    "sigma1i  = tf.Variable(np.float32(3*np.ones([num_gaussians1i,1])))\n",
    "Gauss1i  = tf.concat([mu1i,sigma1i],1)\n",
    "\n",
    "# Random initialization for layer weights\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=1.0)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(1.0, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# Placeholders\n",
    "data0       = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "data0i      = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "data1i      = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "card0       = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "card0i      = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "card1i      = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "label       = tf.placeholder(tf.float32, shape=[None, num_labels])\n",
    "\n",
    "# Treatment of the ordinary persistence diagram in dim 0\n",
    "# Linear -> Batch Norm -> Dropout -> Linear -> ReLu -> Dropout \n",
    "v0  = persistence_vector_module.persistence_vector(data0, card0, Gauss0, nu_tensor0)\n",
    "W01 = weight_variable([150,75])  \n",
    "b01 = bias_variable([75])\n",
    "layer01   = tf.matmul(v0,W01) + b01\n",
    "mean01, variance01 = tf.nn.moments(layer01, axes=[0], keep_dims=True)\n",
    "layer02   = tf.nn.batch_normalization(layer01, mean01, variance01, None, None, 1e-10)\n",
    "layer03   = tf.nn.dropout(layer02, keep_prob)\n",
    "W04 = weight_variable([75,75])  \n",
    "b04 = bias_variable([75])\n",
    "layer04   = tf.matmul(layer03,W04) + b04\n",
    "layer05   = tf.nn.relu(layer04)\n",
    "layer06   = tf.nn.dropout(layer05, keep_prob)\n",
    "\n",
    "# Treatment of the essential persistence diagram in dim 0\n",
    "# Linear -> Batch Norm -> Dropout -> Linear -> ReLu -> Dropout \n",
    "v0i = persistence_vector_essential_module.persistence_vector_essential(data0i, card0i, Gauss0i)\n",
    "W0i1 = weight_variable([50,25])  \n",
    "b0i1 = bias_variable([25])\n",
    "layer0i1   = tf.matmul(v0i,W0i1) + b0i1\n",
    "mean0i1, variance0i1 = tf.nn.moments(layer0i1, axes=[0], keep_dims=True)\n",
    "layer0i2   = tf.nn.batch_normalization(layer0i1, mean0i1, variance0i1, None, None, 1e-10)\n",
    "layer0i3   = tf.nn.dropout(layer0i2, keep_prob)\n",
    "W0i4 = weight_variable([25,25])  \n",
    "b0i4 = bias_variable([25])\n",
    "layer0i4   = tf.matmul(layer0i3,W0i4) + b0i4\n",
    "layer0i5   = tf.nn.relu(layer0i4)\n",
    "layer0i6   = tf.nn.dropout(layer0i5, keep_prob)\n",
    "\n",
    "# Treatment of the essential persistence diagram in dim 1\n",
    "# Linear -> Batch Norm -> Dropout -> Linear -> ReLu -> Dropout \n",
    "v1i = persistence_vector_essential_module.persistence_vector_essential(data1i, card1i, Gauss1i)\n",
    "W1i1 = weight_variable([50,25])  \n",
    "b1i1 = bias_variable([25])\n",
    "layer1i1   = tf.matmul(v1i,W1i1) + b1i1\n",
    "mean1i1, variance1i1 = tf.nn.moments(layer1i1, axes=[0], keep_dims=True)\n",
    "layer1i2   = tf.nn.batch_normalization(layer1i1, mean1i1, variance1i1, None, None, 1e-10)\n",
    "layer1i3   = tf.nn.dropout(layer1i2, keep_prob)\n",
    "W1i4 = weight_variable([25,25])  \n",
    "b1i4 = bias_variable([25])\n",
    "layer1i4   = tf.matmul(layer1i3,W1i4) + b1i4\n",
    "layer1i5   = tf.nn.relu(layer1i4)\n",
    "layer1i6   = tf.nn.dropout(layer1i5, keep_prob)\n",
    "\n",
    "# Merge and treat the three previous branches\n",
    "# Linear -> Batch Norm -> ReLu -> Linear -> Batch Norm -> Dropout -> ReLu -> Linear -> Batch Norm -> ReLu -> Linear -> Batch Norm\n",
    "layer7 = tf.concat([layer06,layer0i6,layer1i6],1)\n",
    "W8 = weight_variable([125,200])  \n",
    "b8 = bias_variable([200])\n",
    "layer8   = tf.matmul(layer7,W8) + b8\n",
    "mean8, variance8 = tf.nn.moments(layer8, axes=[0], keep_dims=True)\n",
    "layer9   = tf.nn.batch_normalization(layer8, mean8, variance8, None, None, 1e-10)\n",
    "layer10  = tf.nn.relu(layer9)\n",
    "W11 = weight_variable([200,100])  \n",
    "b11 = bias_variable([100])\n",
    "layer11   = tf.matmul(layer10,W11) + b11\n",
    "mean11 ,variance11 = tf.nn.moments(layer11, axes=[0], keep_dims=True)\n",
    "layer12   = tf.nn.batch_normalization(layer11, mean11, variance11, None, None, 1e-10)\n",
    "layer13   = tf.nn.dropout(layer12, keep_prob)\n",
    "layer14   = tf.nn.relu(layer13)\n",
    "W15 = weight_variable([100,50])  \n",
    "b15 = bias_variable([50])\n",
    "layer15   = tf.matmul(layer14,W15) + b15\n",
    "mean15, variance15 = tf.nn.moments(layer15, axes=[0], keep_dims=True)\n",
    "layer16   = tf.nn.batch_normalization(layer15, mean15, variance15, None, None, 1e-10)\n",
    "layer17   = tf.nn.relu(layer16)\n",
    "W18 = weight_variable([50,5])  \n",
    "b18 = bias_variable([5])\n",
    "layer18   = tf.matmul(layer17,W18) + b18\n",
    "mean18, variance18 = tf.nn.moments(layer18, axes=[0], keep_dims=True)\n",
    "layer19   = tf.nn.batch_normalization(layer18, mean18, variance18, None, None, 1e-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle data so as to be homogeneous\n",
    "C = list(zip(D0,C0,D0i,C0i,D1i,C1i,labels))\n",
    "shuffle(C)\n",
    "D0,C0,D0i,C0i,D1i,C1i,labels = zip(*C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do the optimization and evaluate on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Accuracy 15.213 %, Test Accuracy 15.1515 % \n",
      "Epoch 1, Train Accuracy 42.7566 %, Test Accuracy 42.6263 % \n",
      "Epoch 2, Train Accuracy 46.3579 %, Test Accuracy 47.4747 % \n",
      "Epoch 3, Train Accuracy 48.523 %, Test Accuracy 49.4949 % \n",
      "Epoch 4, Train Accuracy 50.3653 %, Test Accuracy 50.303 % \n",
      "Epoch 5, Train Accuracy 51.0812 %, Test Accuracy 51.3131 % \n",
      "Epoch 6, Train Accuracy 51.6169 %, Test Accuracy 51.7172 % \n",
      "Epoch 7, Train Accuracy 51.8445 %, Test Accuracy 51.9192 % \n",
      "Epoch 8, Train Accuracy 52.1406 %, Test Accuracy 52.1212 % \n",
      "Epoch 9, Train Accuracy 52.2746 %, Test Accuracy 52.9293 % \n",
      "Epoch 10, Train Accuracy 52.6987 %, Test Accuracy 52.5253 % \n",
      "Epoch 11, Train Accuracy 52.7433 %, Test Accuracy 52.3232 % \n",
      "Epoch 12, Train Accuracy 53.1332 %, Test Accuracy 52.7273 % \n",
      "Epoch 13, Train Accuracy 53.1838 %, Test Accuracy 52.7273 % \n",
      "Epoch 14, Train Accuracy 53.1451 %, Test Accuracy 52.7273 % \n",
      "Epoch 15, Train Accuracy 53.2284 %, Test Accuracy 52.7273 % \n",
      "Epoch 16, Train Accuracy 53.5692 %, Test Accuracy 52.9293 % \n",
      "Epoch 17, Train Accuracy 53.8148 %, Test Accuracy 52.7273 % \n",
      "Epoch 18, Train Accuracy 53.7195 %, Test Accuracy 53.1313 % \n",
      "Epoch 19, Train Accuracy 54.0826 %, Test Accuracy 52.5253 % \n",
      "Epoch 20, Train Accuracy 54.0156 %, Test Accuracy 52.1212 % \n",
      "Epoch 21, Train Accuracy 54.2002 %, Test Accuracy 52.5253 % \n",
      "Epoch 22, Train Accuracy 54.3178 %, Test Accuracy 53.1313 % \n",
      "Epoch 23, Train Accuracy 54.3728 %, Test Accuracy 53.5354 % \n",
      "Epoch 24, Train Accuracy 54.3847 %, Test Accuracy 53.7374 % \n",
      "Epoch 25, Train Accuracy 54.4963 %, Test Accuracy 52.9293 % \n",
      "Epoch 26, Train Accuracy 54.6303 %, Test Accuracy 52.7273 % \n",
      "Epoch 27, Train Accuracy 54.7642 %, Test Accuracy 53.1313 % \n",
      "Epoch 28, Train Accuracy 54.9428 %, Test Accuracy 53.9394 % \n",
      "Epoch 29, Train Accuracy 55.1213 %, Test Accuracy 53.3333 % \n",
      "Test Accuracy = 53.7374 % "
     ]
    }
   ],
   "source": [
    "# Set sub_ratio to x > 1 if you want to train and test on a sub dataset\n",
    "sub_ratio      = 1\n",
    "\n",
    "# Use 90% of data as training\n",
    "training_ratio = 0.9\n",
    "training_size  = np.round(training_ratio*num_pts/sub_ratio).astype(np.int)\n",
    "\n",
    "# Split data into batches of size 128 and optimize over 30 epochs \n",
    "batch_size     = 128\n",
    "nb_epoch       = 30\n",
    "\n",
    "# Compute number of batches\n",
    "if training_size % batch_size == 0:\n",
    "    num_batches = training_size/batch_size\n",
    "else:\n",
    "    num_batches = training_size/batch_size + 1 \n",
    "    \n",
    "# Define accuracy\n",
    "correct_prediction    = tf.equal(tf.argmax(layer19, 1), tf.argmax(label, 1))\n",
    "accuracy              = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Optimize cross entropy loss with Gradient Descent and decaying learning rate\n",
    "cross_entropy         = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=label, logits=layer19))\n",
    "global_step           = tf.Variable(0, trainable=False)\n",
    "learning_rate         = tf.train.exponential_decay(0.1, global_step, 25*num_batches, 0.5, staircase=True)\n",
    "learning              = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "learning_step         = learning.minimize(cross_entropy, global_step=global_step)\n",
    "gradmu0               = learning.compute_gradients(cross_entropy, mu0)\n",
    "\n",
    "# Define training batches\n",
    "chunked_data = []\n",
    "\n",
    "for i in range(0,training_size,batch_size):\n",
    "    \n",
    "    if (training_size < i + batch_size):\n",
    "        num_pts_in_batch = training_size - i\n",
    "    else:\n",
    "        num_pts_in_batch = batch_size\n",
    "    \n",
    "    batch_labels      = []        \n",
    "    batch_data0       = []\n",
    "    batch_card0       = []\n",
    "    batch_data0i      = []\n",
    "    batch_card0i      = []\n",
    "    batch_data1i      = []\n",
    "    batch_card1i      = []\n",
    "    for j in range(num_pts_in_batch):\n",
    "        batch_labels.append(labels[i+j])\n",
    "        batch_data0.append(D0[i+j])\n",
    "        batch_card0.append(C0[i+j])\n",
    "        batch_data0i.append(D0i[i+j])\n",
    "        batch_card0i.append(C0i[i+j])\n",
    "        batch_data1i.append(D1i[i+j])\n",
    "        batch_card1i.append(C1i[i+j])\n",
    "        \n",
    "    # Reshape data for dimensionality agreement with network \n",
    "    chunked_data.append((batch_labels, np.concatenate(batch_data0, 0), \n",
    "                                       np.reshape(batch_card0, (-1,1)), \n",
    "                                       np.reshape(np.concatenate(batch_data0i,0),(-1,1)), \n",
    "                                       np.reshape(batch_card0i,(-1,1)),\n",
    "                                       np.reshape(np.concatenate(batch_data1i,0),(-1,1)),\n",
    "                                       np.reshape(batch_card1i,(-1,1))))\n",
    "\n",
    "# Define test set\n",
    "test_labels      = []        \n",
    "test_data0       = []\n",
    "test_card0       = []\n",
    "test_data0i      = []\n",
    "test_card0i      = []\n",
    "test_data1i      = []\n",
    "test_card1i      = []\n",
    "for i in range(training_size,num_pts/sub_ratio):      \n",
    "    test_labels.append(labels[i])\n",
    "    test_data0.append(D0[i])\n",
    "    test_card0.append(C0[i])\n",
    "    test_data0i.append(D0i[i])\n",
    "    test_card0i.append(C0i[i])\n",
    "    test_data1i.append(D1i[i])\n",
    "    test_card1i.append(C1i[i])\n",
    "    \n",
    "# Reshape data for dimensionality agreement with network \n",
    "test_data0  = np.concatenate(test_data0,  0)\n",
    "test_card0  = np.reshape(test_card0, (-1,1))\n",
    "test_data0i = np.reshape(np.concatenate(test_data0i, 0),(-1,1))\n",
    "test_card0i = np.reshape(test_card0i, (-1,1))\n",
    "test_data1i = np.reshape(np.concatenate(test_data1i, 0),(-1,1))\n",
    "test_card1i = np.reshape(test_card1i, (-1,1))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Initialize\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # For each epoch\n",
    "    for ep in range(nb_epoch):\n",
    "     \n",
    "        # Compute training accuracy   \n",
    "        acc = 0\n",
    "        for i in range(0,num_batches):\n",
    "            acc += accuracy.eval(feed_dict={label:  chunked_data[i][0], \n",
    "                                            data0:  chunked_data[i][1],\n",
    "                                            card0:  chunked_data[i][2],\n",
    "                                            data0i: chunked_data[i][3],\n",
    "                                            card0i: chunked_data[i][4],\n",
    "                                            data1i: chunked_data[i][5],\n",
    "                                            card1i: chunked_data[i][6]})\n",
    "        acc /= num_batches\n",
    "        \n",
    "        # Compute test accuracy\n",
    "        acc_test = accuracy.eval(feed_dict={label:  test_labels, \n",
    "                                            data0:  test_data0,\n",
    "                                            card0:  test_card0,\n",
    "                                            data0i: test_data0i,\n",
    "                                            card0i: test_card0i,\n",
    "                                            data1i: test_data1i,\n",
    "                                            card1i: test_card1i})\n",
    "        sys.stdout.write(\"Epoch %d, Train Accuracy %g %s, Test Accuracy %g %s \\n\" % \n",
    "                         (ep,100*acc,'%',100*acc_test,'%')) \n",
    "        \n",
    "        # Optimize each batch\n",
    "        for i in range(0,num_batches):\n",
    "            learning_step.run(feed_dict={label:  chunked_data[i][0], \n",
    "                                         data0:  chunked_data[i][1],\n",
    "                                         card0:  chunked_data[i][2],\n",
    "                                         data0i: chunked_data[i][3],\n",
    "                                         card0i: chunked_data[i][4],\n",
    "                                         data1i: chunked_data[i][5],\n",
    "                                         card1i: chunked_data[i][6]})\n",
    "\n",
    "    # Compute final accuracy\n",
    "    acc_test = accuracy.eval(feed_dict={label:  test_labels, \n",
    "                                        data0:  test_data0,\n",
    "                                        card0:  test_card0,\n",
    "                                        data0i: test_data0i,\n",
    "                                        card0i: test_card0i,\n",
    "                                        data1i: test_data1i,\n",
    "                                        card1i: test_card1i})\n",
    "    sys.stdout.write(\"Test Accuracy = %g %s \" % (100*acc_test, '%'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
